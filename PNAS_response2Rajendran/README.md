
# Reply to Rajendran and Schnupp: Frequency-tagging is sensitive to the temporal structure of signals<p align="center">Tomas Lenc<sup>a</sup>, Peter E. Keller<sup>a</sup>, Manuel Varlet<sup>a</sup>, Sylvie Nozaradan<sup>a,b,s</sup></p>
<sup>a</sup> MARCS Institute for Brain, Behaviour and Development Western Sydney University, Locked Bag 1797 Penrith NSW 2751 Australia 
<sup>b</sup> Institute of Neuroscience (IONS), Université catholique de Louvain (UCL), Avenue Mounier 53 Woluwe-Saint-Lambert 1200 Belgium <sup>c</sup> International Laboratory for Brain, Music and Sound Research (BRAMS), Pavillon 1420 Mont Royal, FAS – Département de psychologie, CP 6128, succ. Centre Ville, Montréal, QC H3C 3J7 &nbsp;  __<p align="center">Appendix</p>__

&nbsp;  
<b>Greater relative prominence of the beat and meter frequencies with low-tone in the frequency domain corresponds to periodic amplitude increase locked to the beat</b>
To evaluate possible phase differences in the EEG data of (1), we used 9 frontocentral channels (compared to all 64 channels in the original analysis), as the phase might be reversed on the opposite side of the dipole (temporo-occipital channels). The original results with z-scored amplitudes at meter frequencies still hold only with this subset of channels (interaction Tone x Rhythm, F = 5.53, P = 0.02, mixed model fitted with lme4 package for R (2), [R code](https://github.com/TomasLenc/XPLOWHIGH/blob/master/PNAS_response2Rajendran/amplitude_meterZscore_avgFront.R)). From this pool of channels, we measured the phase values at the 12 frequencies of interest from the EEG spectrum of each participant and condition. These phase values were then compared across conditions by fitting a set of nested mixed effects models using bpnreg package for R (3) with factors Frequency (the 12 frequencies of interest), Rhythm (unsyncopated, syncopated) and Tone (low, high). The best model accounting for the data did not involve the factor Tone (see Table 1 in (4), [R code](https://github.com/TomasLenc/XPLOWHIGH/blob/master/PNAS_response2Rajendran/phase_fitNestedModels.R)), which suggests no phase differences between the low-tone and high-tone conditions. 
This lack of difference was not due to generally low phase locking of the responses (i.e. random phase across trials). We calculated mean vector length (r) across the 8 trials for each of the 12 frequencies separately for each condition and participant. We did the same repeatedly for 12 frequencies extracted from neighboring frequency bins (bins -20 to -1 and +1 to +20 around each frequency of interest). We calculated the mean r across all “bin-shifted” versions to serve as a baseline: if phases at frequency-bins centered at the exact frequencies of interest were random, their r would not be significantly different from the baseline. There was an overall greater r for the 12 frequencies of interest compared to baseline (F = 277.64, P < 0.0001, mixed model fitted with lme4, [R code](https://github.com/TomasLenc/XPLOWHIGH/blob/master/PNAS_response2Rajendran/phase_meanVectorLength.R)), and as expected, this contrast depended on the amplitude of the particular frequency, which differed across the two rhythms (Frequency x Rhythm x Baseline interaction, F = 9.88, P < 0.0001). This indicates that the lack of phase difference between low-tone and high-tone conditions described above is unlikely to be due to overall inconsistent phases across the 12 frequencies of interest in the neural responses. 
<b>Frequency-tagging is sensitive to the neural locking to the beat and meter</b>
To test whether frequency-tagging can capture differences in neural locking to the beat and meter, we simulated 50 experiments with a standard and jittered condition ([Matlab code](https://github.com/TomasLenc/XPLOWHIGH/blob/master/PNAS_response2Rajendran/meterFreqNoise_simulateExp.m)). Both conditions assumed that the brain responded at the frequencies present in the stimulus with the same magnitudes as in the stimulus. The phases of the spectral components were randomly generated separately for each participant and condition. In the jittered condition, noise was introduced into the instantaneous frequency of the spectral components corresponding to the meter-related frequencies, to generate lower locking of the response at the beat and meter frequencies. All simulated experiments yielded significant differences between the two conditions (paired ttest) in phase locking value at the beat frequency, but also in mean z-scored amplitude at meter-related frequencies, as revealed by frequency-tagging.  1. 	Lenc T, Keller PE, Varlet M, Nozaradan S (2018) Neural tracking of the musical beat is enhanced by low-frequency sounds. Proc Natl Acad Sci 115(32):8221–8226.2. 	Bates D, Maechler M, Bolker B, Walker S (2015) lme4: Linear mixed-effects models using Eigen and S4. R package version 1.1-9 http://CRAN. R-project org/package= lme4.3. 	Cremers J, Klugkist I (2018) One direction? A tutorial for circular data using R with examples in cognitive psychology. Front Psychol 9:2040.4.	Reply to Rajendran and Schnupp: Frequency-tagging is sensitive to the temporal structure of signals. Proc Natl Acad Sci In press.